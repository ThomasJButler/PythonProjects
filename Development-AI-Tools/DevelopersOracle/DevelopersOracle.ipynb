{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DevelopersOracle: AI-Powered Development Assistant\n",
    "\n",
    "An intelligent development assistant that integrates with multiple platforms to enhance productivity through automated analysis, task estimation, and intelligent recommendations.\n",
    "\n",
    "## Overview\n",
    "\n",
    "DevelopersOracle combines artificial intelligence, machine learning, and platform integrations to provide:\n",
    "- Intelligent code analysis and commit message generation\n",
    "- Project management integration with JIRA, GitHub, GitLab, and Bitbucket\n",
    "- Task estimation using machine learning models\n",
    "- Sentiment analysis of project communications\n",
    "- Automated development workflow assistance\n",
    "\n",
    "## Features Demonstration\n",
    "\n",
    "**Note**: This notebook provides demonstrations with mock data for educational purposes. In production, you would need proper API credentials and configuration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import datetime\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"✅ DevelopersOracle libraries imported successfully!\")\n",
    "print(\"📝 Note: This demonstration uses mock data for educational purposes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Core DevelopersOracle Class (Simplified)\n",
    "\n",
    "Here's a simplified version of the main class for demonstration purposes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DevelopersOracleDemo:\n",
    "    \"\"\"Simplified DevelopersOracle for Jupyter demonstration.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.time_tracker = {}\n",
    "        self.estimation_model = None\n",
    "        self.setup_ml_models()\n",
    "        print(\"🔮 DevelopersOracle Demo initialised successfully!\")\n",
    "    \n",
    "    def setup_ml_models(self):\n",
    "        \"\"\"Initialise machine learning models for task estimation.\"\"\"\n",
    "        self.estimation_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        \n",
    "        # Generate synthetic training data for demonstration\n",
    "        X, y = self.generate_mock_training_data()\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        self.estimation_model.fit(X_train_scaled, y_train)\n",
    "        self.scaler = scaler\n",
    "        \n",
    "        # Evaluate model\n",
    "        y_pred = self.estimation_model.predict(X_test_scaled)\n",
    "        self.model_r2 = r2_score(y_test, y_pred)\n",
    "        self.model_rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        \n",
    "        print(f\"📊 ML Model trained - R² Score: {self.model_r2:.3f}, RMSE: {self.model_rmse:.2f}\")\n",
    "    \n",
    "    def generate_mock_training_data(self, n_samples=200):\n",
    "        \"\"\"Generate synthetic training data for task estimation.\"\"\"\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        # Features: [complexity, lines_of_code, num_files, team_experience, priority]\n",
    "        complexity = np.random.randint(1, 6, n_samples)  # 1-5 scale\n",
    "        lines_of_code = np.random.exponential(200, n_samples)  # Exponential distribution\n",
    "        num_files = np.random.poisson(5, n_samples)  # Poisson distribution\n",
    "        team_experience = np.random.uniform(1, 10, n_samples)  # 1-10 scale\n",
    "        priority = np.random.randint(1, 4, n_samples)  # 1-3 scale\n",
    "        \n",
    "        X = np.column_stack([complexity, lines_of_code, num_files, team_experience, priority])\n",
    "        \n",
    "        # Target: estimated hours (with some realistic relationships)\n",
    "        y = (complexity * 3 + \n",
    "             lines_of_code * 0.01 + \n",
    "             num_files * 1.5 + \n",
    "             (10 - team_experience) * 2 + \n",
    "             priority * 2 + \n",
    "             np.random.normal(0, 2, n_samples))  # Add noise\n",
    "        \n",
    "        y = np.clip(y, 1, 100)  # Clip to reasonable range\n",
    "        \n",
    "        return X, y\n",
    "    \n",
    "    def estimate_task_time(self, complexity, lines_of_code, num_files, team_experience, priority):\n",
    "        \"\"\"Estimate task completion time using the trained model.\"\"\"\n",
    "        features = np.array([[complexity, lines_of_code, num_files, team_experience, priority]])\n",
    "        features_scaled = self.scaler.transform(features)\n",
    "        \n",
    "        estimated_hours = self.estimation_model.predict(features_scaled)[0]\n",
    "        confidence = min(self.model_r2 * 100, 95)  # Cap confidence at 95%\n",
    "        \n",
    "        return {\n",
    "            'estimated_hours': round(estimated_hours, 1),\n",
    "            'confidence': round(confidence, 1),\n",
    "            'range': (round(estimated_hours * 0.8, 1), round(estimated_hours * 1.2, 1))\n",
    "        }\n",
    "    \n",
    "    def analyse_sentiment(self, text):\n",
    "        \"\"\"Simple sentiment analysis (mocked for demonstration).\"\"\"\n",
    "        positive_words = ['good', 'great', 'excellent', 'amazing', 'fantastic', 'love', 'perfect', 'awesome', 'brilliant']\n",
    "        negative_words = ['bad', 'terrible', 'awful', 'hate', 'horrible', 'worst', 'disaster', 'frustrated', 'annoying']\n",
    "        \n",
    "        text_lower = text.lower()\n",
    "        \n",
    "        positive_count = sum(1 for word in positive_words if word in text_lower)\n",
    "        negative_count = sum(1 for word in negative_words if word in text_lower)\n",
    "        \n",
    "        if positive_count > negative_count:\n",
    "            return {'sentiment': 'Positive', 'score': min(0.5 + positive_count * 0.2, 1.0)}\n",
    "        elif negative_count > positive_count:\n",
    "            return {'sentiment': 'Negative', 'score': max(-0.5 - negative_count * 0.2, -1.0)}\n",
    "        else:\n",
    "            return {'sentiment': 'Neutral', 'score': 0.0}\n",
    "    \n",
    "    def generate_commit_message(self, changes_description):\n",
    "        \"\"\"Generate a commit message based on changes (mocked).\"\"\"\n",
    "        templates = {\n",
    "            'feature': \"feat: add {feature}\",\n",
    "            'fix': \"fix: resolve {issue}\",\n",
    "            'update': \"update: improve {component}\",\n",
    "            'refactor': \"refactor: restructure {module}\",\n",
    "            'docs': \"docs: update {documentation}\",\n",
    "            'test': \"test: add tests for {functionality}\"\n",
    "        }\n",
    "        \n",
    "        # Simple keyword-based categorisation\n",
    "        changes_lower = changes_description.lower()\n",
    "        \n",
    "        if any(word in changes_lower for word in ['add', 'new', 'create', 'implement']):\n",
    "            return templates['feature'].format(feature=changes_description)\n",
    "        elif any(word in changes_lower for word in ['fix', 'bug', 'error', 'issue']):\n",
    "            return templates['fix'].format(issue=changes_description)\n",
    "        elif any(word in changes_lower for word in ['update', 'improve', 'enhance']):\n",
    "            return templates['update'].format(component=changes_description)\n",
    "        elif any(word in changes_lower for word in ['refactor', 'restructure', 'reorganise']):\n",
    "            return templates['refactor'].format(module=changes_description)\n",
    "        elif any(word in changes_lower for word in ['doc', 'readme', 'comment']):\n",
    "            return templates['docs'].format(documentation=changes_description)\n",
    "        else:\n",
    "            return f\"chore: {changes_description}\"\n",
    "\n",
    "# Initialise the demo class\n",
    "oracle = DevelopersOracleDemo()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interactive Task Estimation Tool\n",
    "\n",
    "Use machine learning to estimate development task completion time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive task estimation widget\n",
    "complexity_slider = widgets.IntSlider(\n",
    "    value=3, min=1, max=5, step=1,\n",
    "    description='Complexity:',\n",
    "    tooltip='Task complexity (1=Very Simple, 5=Very Complex)'\n",
    ")\n",
    "\n",
    "lines_slider = widgets.IntSlider(\n",
    "    value=150, min=10, max=1000, step=10,\n",
    "    description='Lines of Code:',\n",
    "    tooltip='Estimated lines of code to be modified/added'\n",
    ")\n",
    "\n",
    "files_slider = widgets.IntSlider(\n",
    "    value=3, min=1, max=20, step=1,\n",
    "    description='Files Count:',\n",
    "    tooltip='Number of files to be modified'\n",
    ")\n",
    "\n",
    "experience_slider = widgets.FloatSlider(\n",
    "    value=6.5, min=1.0, max=10.0, step=0.5,\n",
    "    description='Team Experience:',\n",
    "    tooltip='Team experience level (1=Beginner, 10=Expert)'\n",
    ")\n",
    "\n",
    "priority_slider = widgets.IntSlider(\n",
    "    value=2, min=1, max=3, step=1,\n",
    "    description='Priority:',\n",
    "    tooltip='Task priority (1=Low, 2=Medium, 3=High)'\n",
    ")\n",
    "\n",
    "estimate_button = widgets.Button(\n",
    "    description='Estimate Task Time',\n",
    "    button_style='success',\n",
    "    tooltip='Click to get ML-based time estimation'\n",
    ")\n",
    "\n",
    "estimation_output = widgets.HTML(value=\"<p>Adjust parameters and click estimate</p>\")\n",
    "\n",
    "def on_estimate_click(b):\n",
    "    estimate = oracle.estimate_task_time(\n",
    "        complexity_slider.value,\n",
    "        lines_slider.value,\n",
    "        files_slider.value,\n",
    "        experience_slider.value,\n",
    "        priority_slider.value\n",
    "    )\n",
    "    \n",
    "    output_html = f\"\"\"\n",
    "    <div style='background-color: #f0f8ff; padding: 15px; border-radius: 10px; border-left: 5px solid #007acc;'>\n",
    "        <h3 style='color: #007acc; margin-top: 0;'>📊 Task Estimation Results</h3>\n",
    "        <p><strong>Estimated Time:</strong> {estimate['estimated_hours']} hours</p>\n",
    "        <p><strong>Confidence Level:</strong> {estimate['confidence']}%</p>\n",
    "        <p><strong>Range:</strong> {estimate['range'][0]} - {estimate['range'][1]} hours</p>\n",
    "        <p style='font-size: 0.9em; color: #666;'>\n",
    "            💡 <em>This estimate is based on historical data patterns and team experience.</em>\n",
    "        </p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "    estimation_output.value = output_html\n",
    "\n",
    "estimate_button.on_click(on_estimate_click)\n",
    "\n",
    "# Display the estimation tool\n",
    "estimation_widget = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>🔮 AI Task Estimation Tool</h2>\"),\n",
    "    complexity_slider,\n",
    "    lines_slider,\n",
    "    files_slider,\n",
    "    experience_slider,\n",
    "    priority_slider,\n",
    "    estimate_button,\n",
    "    estimation_output\n",
    "])\n",
    "\n",
    "display(estimation_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis Tool\n",
    "\n",
    "Analyse the sentiment of project communications:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sentiment analysis widget\n",
    "text_area = widgets.Textarea(\n",
    "    value='This new feature is absolutely brilliant! The team has done an amazing job implementing it.',\n",
    "    placeholder='Enter text to analyse sentiment...',\n",
    "    description='Text:',\n",
    "    layout=widgets.Layout(width='100%', height='100px')\n",
    ")\n",
    "\n",
    "analyse_button = widgets.Button(\n",
    "    description='Analyse Sentiment',\n",
    "    button_style='info',\n",
    "    tooltip='Click to analyse text sentiment'\n",
    ")\n",
    "\n",
    "sentiment_output = widgets.HTML(value=\"<p>Enter text and click analyse</p>\")\n",
    "\n",
    "def on_analyse_click(b):\n",
    "    text = text_area.value\n",
    "    if text.strip():\n",
    "        result = oracle.analyse_sentiment(text)\n",
    "        \n",
    "        # Determine colour based on sentiment\n",
    "        if result['sentiment'] == 'Positive':\n",
    "            colour = '#28a745'  # Green\n",
    "            emoji = '😊'\n",
    "        elif result['sentiment'] == 'Negative':\n",
    "            colour = '#dc3545'  # Red\n",
    "            emoji = '😞'\n",
    "        else:\n",
    "            colour = '#ffc107'  # Yellow\n",
    "            emoji = '😐'\n",
    "        \n",
    "        output_html = f\"\"\"\n",
    "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 10px; border-left: 5px solid {colour};'>\n",
    "            <h3 style='color: {colour}; margin-top: 0;'>{emoji} Sentiment Analysis Results</h3>\n",
    "            <p><strong>Sentiment:</strong> {result['sentiment']}</p>\n",
    "            <p><strong>Score:</strong> {result['score']:.2f}</p>\n",
    "            <div style='background-color: #e9ecef; padding: 10px; border-radius: 5px; margin-top: 10px;'>\n",
    "                <small><strong>Analysed Text:</strong> \"{text[:100]}{'...' if len(text) > 100 else ''}\"</small>\n",
    "            </div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        sentiment_output.value = output_html\n",
    "    else:\n",
    "        sentiment_output.value = \"<p style='color: orange;'>⚠️ Please enter some text to analyse</p>\"\n",
    "\n",
    "analyse_button.on_click(on_analyse_click)\n",
    "\n",
    "# Display sentiment analysis tool\n",
    "sentiment_widget = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>💭 Sentiment Analysis Tool</h2>\"),\n",
    "    text_area,\n",
    "    analyse_button,\n",
    "    sentiment_output\n",
    "])\n",
    "\n",
    "display(sentiment_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commit Message Generator\n",
    "\n",
    "Generate semantic commit messages based on change descriptions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create commit message generator widget\n",
    "changes_input = widgets.Text(\n",
    "    value='user authentication system',\n",
    "    placeholder='Describe your changes...',\n",
    "    description='Changes:',\n",
    "    style={'description_width': 'initial'},\n",
    "    layout=widgets.Layout(width='100%')\n",
    ")\n",
    "\n",
    "generate_button = widgets.Button(\n",
    "    description='Generate Commit Message',\n",
    "    button_style='primary',\n",
    "    tooltip='Generate semantic commit message'\n",
    ")\n",
    "\n",
    "commit_output = widgets.HTML(value=\"<p>Describe your changes and click generate</p>\")\n",
    "\n",
    "def on_generate_click(b):\n",
    "    changes = changes_input.value\n",
    "    if changes.strip():\n",
    "        commit_msg = oracle.generate_commit_message(changes)\n",
    "        \n",
    "        output_html = f\"\"\"\n",
    "        <div style='background-color: #f8f9fa; padding: 15px; border-radius: 10px; border-left: 5px solid #6f42c1;'>\n",
    "            <h3 style='color: #6f42c1; margin-top: 0;'>📝 Generated Commit Message</h3>\n",
    "            <div style='background-color: #2d3748; color: #e2e8f0; padding: 10px; border-radius: 5px; font-family: monospace;'>\n",
    "                <code>{commit_msg}</code>\n",
    "            </div>\n",
    "            <p style='margin-top: 10px; font-size: 0.9em; color: #666;'>\n",
    "                💡 <em>Following conventional commits specification</em>\n",
    "            </p>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "        commit_output.value = output_html\n",
    "    else:\n",
    "        commit_output.value = \"<p style='color: orange;'>⚠️ Please describe your changes</p>\"\n",
    "\n",
    "generate_button.on_click(on_generate_click)\n",
    "\n",
    "# Display commit message generator\n",
    "commit_widget = widgets.VBox([\n",
    "    widgets.HTML(\"<h2>🔀 Commit Message Generator</h2>\"),\n",
    "    changes_input,\n",
    "    generate_button,\n",
    "    commit_output\n",
    "])\n",
    "\n",
    "display(commit_widget)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Performance Visualisation\n",
    "\n",
    "Analyse the performance of our task estimation model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate test data for visualisation\n",
    "X_test, y_test = oracle.generate_mock_training_data(50)\n",
    "X_test_scaled = oracle.scaler.transform(X_test)\n",
    "y_pred = oracle.estimation_model.predict(X_test_scaled)\n",
    "\n",
    "# Create comprehensive visualisation\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "\n",
    "# 1. Actual vs Predicted\n",
    "axes[0, 0].scatter(y_test, y_pred, alpha=0.6, color='#007acc')\n",
    "axes[0, 0].plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--', lw=2)\n",
    "axes[0, 0].set_xlabel('Actual Hours')\n",
    "axes[0, 0].set_ylabel('Predicted Hours')\n",
    "axes[0, 0].set_title('Actual vs Predicted Task Duration')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Add R² score annotation\n",
    "axes[0, 0].text(0.05, 0.95, f'R² = {oracle.model_r2:.3f}', \n",
    "                transform=axes[0, 0].transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8))\n",
    "\n",
    "# 2. Feature Importance\n",
    "feature_names = ['Complexity', 'Lines of Code', 'File Count', 'Team Experience', 'Priority']\n",
    "feature_importance = oracle.estimation_model.feature_importances_\n",
    "colors = plt.cm.Set3(np.linspace(0, 1, len(feature_names)))\n",
    "\n",
    "bars = axes[0, 1].bar(feature_names, feature_importance, color=colors)\n",
    "axes[0, 1].set_title('Feature Importance in Task Estimation')\n",
    "axes[0, 1].set_ylabel('Importance')\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, importance in zip(bars, feature_importance):\n",
    "    height = bar.get_height()\n",
    "    axes[0, 1].text(bar.get_x() + bar.get_width()/2., height + 0.01,\n",
    "                    f'{importance:.3f}', ha='center', va='bottom')\n",
    "\n",
    "# 3. Residuals Plot\n",
    "residuals = y_test - y_pred\n",
    "axes[1, 0].scatter(y_pred, residuals, alpha=0.6, color='#28a745')\n",
    "axes[1, 0].axhline(y=0, color='r', linestyle='--')\n",
    "axes[1, 0].set_xlabel('Predicted Hours')\n",
    "axes[1, 0].set_ylabel('Residuals')\n",
    "axes[1, 0].set_title('Residuals Plot')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# 4. Error Distribution\n",
    "axes[1, 1].hist(residuals, bins=15, alpha=0.7, color='#ff6b6b', edgecolor='black')\n",
    "axes[1, 1].axvline(x=0, color='r', linestyle='--', linewidth=2)\n",
    "axes[1, 1].set_xlabel('Prediction Error (hours)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Distribution of Prediction Errors')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Add statistics text\n",
    "mean_error = np.mean(residuals)\n",
    "std_error = np.std(residuals)\n",
    "axes[1, 1].text(0.05, 0.95, f'Mean Error: {mean_error:.2f}\\nStd Error: {std_error:.2f}',\n",
    "                transform=axes[1, 1].transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "                verticalalignment='top')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('DevelopersOracle: ML Model Performance Analysis', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Print model summary\n",
    "print(\"\\n📊 Model Performance Summary:\")\n",
    "print(\"=\" * 40)\n",
    "print(f\"R² Score: {oracle.model_r2:.3f}\")\n",
    "print(f\"RMSE: {oracle.model_rmse:.2f} hours\")\n",
    "print(f\"Mean Absolute Error: {np.mean(np.abs(residuals)):.2f} hours\")\n",
    "print(f\"Prediction Accuracy (±20%): {np.mean(np.abs(residuals/y_test) <= 0.2)*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Analytics Dashboard\n",
    "\n",
    "Simulate project analytics with mock data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate mock project data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=90, freq='D')\n",
    "\n",
    "# Simulate development metrics\n",
    "commits_per_day = np.random.poisson(3, len(dates))\n",
    "lines_changed = np.random.exponential(150, len(dates))\n",
    "bugs_found = np.random.poisson(1, len(dates))\n",
    "bugs_fixed = np.random.poisson(1.2, len(dates))\n",
    "\n",
    "# Calculate cumulative metrics\n",
    "cumulative_commits = np.cumsum(commits_per_day)\n",
    "bug_backlog = np.cumsum(bugs_found - bugs_fixed)\n",
    "bug_backlog = np.maximum(bug_backlog, 0)  # Can't have negative bugs\n",
    "\n",
    "# Create project analytics dashboard\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# 1. Daily Commits\n",
    "axes[0, 0].plot(dates, commits_per_day, color='#007acc', linewidth=2)\n",
    "axes[0, 0].fill_between(dates, commits_per_day, alpha=0.3, color='#007acc')\n",
    "axes[0, 0].set_title('Daily Commits')\n",
    "axes[0, 0].set_ylabel('Number of Commits')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "axes[0, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 2. Cumulative Progress\n",
    "axes[0, 1].plot(dates, cumulative_commits, color='#28a745', linewidth=3)\n",
    "axes[0, 1].set_title('Cumulative Commits')\n",
    "axes[0, 1].set_ylabel('Total Commits')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "axes[0, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 3. Code Changes\n",
    "axes[0, 2].bar(dates[::7], lines_changed[::7], color='#ff6b6b', alpha=0.7, width=5)\n",
    "axes[0, 2].set_title('Weekly Lines Changed')\n",
    "axes[0, 2].set_ylabel('Lines of Code')\n",
    "axes[0, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 4. Bug Tracking\n",
    "axes[1, 0].plot(dates, bugs_found, label='Bugs Found', color='#dc3545', linewidth=2)\n",
    "axes[1, 0].plot(dates, bugs_fixed, label='Bugs Fixed', color='#28a745', linewidth=2)\n",
    "axes[1, 0].set_title('Bug Discovery vs Resolution')\n",
    "axes[1, 0].set_ylabel('Number of Bugs')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "axes[1, 0].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 5. Bug Backlog\n",
    "axes[1, 1].fill_between(dates, bug_backlog, alpha=0.5, color='#ffc107')\n",
    "axes[1, 1].plot(dates, bug_backlog, color='#ff8c00', linewidth=2)\n",
    "axes[1, 1].set_title('Bug Backlog Trend')\n",
    "axes[1, 1].set_ylabel('Open Bugs')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "axes[1, 1].tick_params(axis='x', rotation=45)\n",
    "\n",
    "# 6. Team Productivity (mock sentiment)\n",
    "productivity_score = 0.7 + 0.3 * np.sin(np.arange(len(dates)) * 2 * np.pi / 30) + np.random.normal(0, 0.1, len(dates))\n",
    "productivity_score = np.clip(productivity_score, 0, 1)\n",
    "\n",
    "color_map = ['red' if p < 0.5 else 'orange' if p < 0.7 else 'green' for p in productivity_score]\n",
    "axes[1, 2].scatter(dates[::3], productivity_score[::3], c=[color_map[i] for i in range(0, len(dates), 3)], alpha=0.7)\n",
    "axes[1, 2].plot(dates, productivity_score, color='gray', alpha=0.5)\n",
    "axes[1, 2].set_title('Team Sentiment Score')\n",
    "axes[1, 2].set_ylabel('Sentiment (0-1)')\n",
    "axes[1, 2].set_ylim(0, 1)\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "axes[1, 2].tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.suptitle('DevelopersOracle: Project Analytics Dashboard', fontsize=16, y=1.02)\n",
    "plt.show()\n",
    "\n",
    "# Generate summary statistics\n",
    "print(\"\\n📈 Project Summary (Last 90 Days):\")\n",
    "print(\"=\" * 45)\n",
    "print(f\"Total Commits: {cumulative_commits[-1]}\")\n",
    "print(f\"Average Daily Commits: {np.mean(commits_per_day):.1f}\")\n",
    "print(f\"Total Lines Changed: {np.sum(lines_changed):,.0f}\")\n",
    "print(f\"Bugs Found: {np.sum(bugs_found)}\")\n",
    "print(f\"Bugs Fixed: {np.sum(bugs_fixed)}\")\n",
    "print(f\"Current Bug Backlog: {bug_backlog[-1]}\")\n",
    "print(f\"Average Team Sentiment: {np.mean(productivity_score):.2f}\")\n",
    "print(f\"Code Velocity: {np.sum(lines_changed)/np.sum(commits_per_day):.1f} lines/commit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Educational Examples\n",
    "\n",
    "Explore different scenarios with the AI assistant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example scenarios for task estimation\n",
    "scenarios = [\n",
    "    {\n",
    "        'name': 'Simple Bug Fix',\n",
    "        'complexity': 1,\n",
    "        'lines': 20,\n",
    "        'files': 1,\n",
    "        'experience': 8.0,\n",
    "        'priority': 2\n",
    "    },\n",
    "    {\n",
    "        'name': 'New Feature Implementation',\n",
    "        'complexity': 4,\n",
    "        'lines': 300,\n",
    "        'files': 8,\n",
    "        'experience': 6.0,\n",
    "        'priority': 3\n",
    "    },\n",
    "    {\n",
    "        'name': 'Database Migration',\n",
    "        'complexity': 5,\n",
    "        'lines': 150,\n",
    "        'files': 12,\n",
    "        'experience': 4.0,\n",
    "        'priority': 3\n",
    "    },\n",
    "    {\n",
    "        'name': 'UI Enhancement',\n",
    "        'complexity': 2,\n",
    "        'lines': 100,\n",
    "        'files': 4,\n",
    "        'experience': 7.5,\n",
    "        'priority': 1\n",
    "    }\n",
    "]\n",
    "\n",
    "print(\"🎯 Example Task Estimations:\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "scenario_results = []\n",
    "for scenario in scenarios:\n",
    "    estimate = oracle.estimate_task_time(\n",
    "        scenario['complexity'],\n",
    "        scenario['lines'],\n",
    "        scenario['files'],\n",
    "        scenario['experience'],\n",
    "        scenario['priority']\n",
    "    )\n",
    "    \n",
    "    scenario_results.append({\n",
    "        'name': scenario['name'],\n",
    "        'hours': estimate['estimated_hours'],\n",
    "        'confidence': estimate['confidence']\n",
    "    })\n",
    "    \n",
    "    print(f\"\\n📋 {scenario['name']}:\")\n",
    "    print(f\"   Complexity: {scenario['complexity']}/5\")\n",
    "    print(f\"   Lines of Code: {scenario['lines']}\")\n",
    "    print(f\"   Files: {scenario['files']}\")\n",
    "    print(f\"   Team Experience: {scenario['experience']}/10\")\n",
    "    print(f\"   Priority: {scenario['priority']}/3\")\n",
    "    print(f\"   ⏱️  Estimated Time: {estimate['estimated_hours']} hours\")\n",
    "    print(f\"   📊 Confidence: {estimate['confidence']}%\")\n",
    "    print(f\"   📈 Range: {estimate['range'][0]}-{estimate['range'][1]} hours\")\n",
    "\n",
    "# Visualise scenario comparisons\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "scenario_names = [r['name'] for r in scenario_results]\n",
    "hours = [r['hours'] for r in scenario_results]\n",
    "confidences = [r['confidence'] for r in scenario_results]\n",
    "\n",
    "# Create colour map based on confidence\n",
    "colors = plt.cm.RdYlGn([c/100 for c in confidences])\n",
    "\n",
    "bars = plt.bar(scenario_names, hours, color=colors, alpha=0.8, edgecolor='black')\n",
    "\n",
    "# Add confidence labels on bars\n",
    "for bar, confidence in zip(bars, confidences):\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height + 0.5,\n",
    "             f'{confidence:.1f}%', ha='center', va='bottom', fontweight='bold')\n",
    "\n",
    "plt.title('Task Estimation Comparison Across Different Scenarios')\n",
    "plt.xlabel('Scenario')\n",
    "plt.ylabel('Estimated Hours')\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Add colorbar\n",
    "sm = plt.cm.ScalarMappable(cmap=plt.cm.RdYlGn, norm=plt.Normalize(vmin=0, vmax=100))\n",
    "sm.set_array([])\n",
    "cbar = plt.colorbar(sm)\n",
    "cbar.set_label('Confidence %')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook has demonstrated the key capabilities of DevelopersOracle:\n",
    "\n",
    "### 🔮 AI-Powered Features:\n",
    "- **Task Estimation**: Machine learning-based time prediction with confidence intervals\n",
    "- **Sentiment Analysis**: Analyse team communications and project sentiment\n",
    "- **Commit Message Generation**: Automated semantic commit messages following conventions\n",
    "- **Project Analytics**: Comprehensive dashboard for tracking development metrics\n",
    "\n",
    "### 📊 Key Benefits:\n",
    "- ✅ **Improved Planning**: Data-driven task estimation reduces planning uncertainty\n",
    "- ✅ **Enhanced Communication**: Automated message generation and sentiment tracking\n",
    "- ✅ **Better Insights**: Analytics dashboard provides actionable project insights\n",
    "- ✅ **Increased Productivity**: Automation of routine development tasks\n",
    "\n",
    "### 🛠️ Technical Implementation:\n",
    "- **Machine Learning**: Random Forest regression for task estimation\n",
    "- **Natural Language Processing**: Text analysis for sentiment and commit messages\n",
    "- **Interactive Widgets**: User-friendly interfaces for real-time analysis\n",
    "- **Data Visualisation**: Comprehensive charts and dashboards\n",
    "\n",
    "### 🚀 Production Considerations:\n",
    "In a production environment, DevelopersOracle would:\n",
    "- Integrate with real APIs (GitHub, JIRA, GitLab, etc.)\n",
    "- Use advanced NLP models (transformers, BERT)\n",
    "- Implement proper authentication and security\n",
    "- Scale with containerisation and cloud deployment\n",
    "- Provide real-time data synchronisation\n",
    "\n",
    "The AI assistant represents a powerful tool for modern development teams, combining machine learning with practical development workflows to enhance productivity and decision-making."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}